{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 파이토치로 구현한 CNN\n",
    "CNN은 Convolutional Neural Network의 약자로, 이미지 처리에 특화된 신경망입니다.\n",
    "CNN은 이미지의 특징을 추출하는 컨볼루션층(합성곱)과 풀링층, 그리고 분류를 수행하는 완전연결층으로 구성되어 있습니다.\n",
    "컨볼루션층에서 이미지의 특징을을 추출하고 풀링 계층은 그 필터를 거친 여러 특징중 중요한 특징들을 골라냅니다.\n",
    "덜 중요한 특징은 버림으로써 이미지의 차원을 감소시켜 비용을 낮춥니다.\n",
    "이런 층들로 만들어진 CNN이 궁극적으로 하는 것은 이미지에서 특징을 추출하는 필터를 학습시키는 것입니다.\n",
    "이번에는 파이토치로 CNN을 구현하고, Fashion MNIST 데이터셋을 분류하는 실습을 진행해보겠습니다.\n",
    "## 3-1. Fashion MNIST 데이터셋 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.290603\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.465654\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.122236\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.022029\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.831336\n",
      "[1] Test Loss: 0.7764, Accuracy: 71.08%\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.903501\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.995250\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.699608\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.636357\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.839400\n",
      "[2] Test Loss: 0.6970, Accuracy: 73.44%\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.677885\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.835962\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.720325\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.708698\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.634640\n",
      "[3] Test Loss: 0.6219, Accuracy: 77.01%\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.633735\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.794682\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.723401\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.800053\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.622616\n",
      "[4] Test Loss: 0.5775, Accuracy: 79.25%\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.537906\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.444881\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.970857\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.702079\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.504123\n",
      "[5] Test Loss: 0.5503, Accuracy: 79.83%\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.565292\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.744661\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.666888\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.681844\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.596974\n",
      "[6] Test Loss: 0.5407, Accuracy: 79.87%\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.558868\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.390458\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.460622\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.409435\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.348587\n",
      "[7] Test Loss: 0.5167, Accuracy: 81.47%\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.745244\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.498714\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.738662\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.698509\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.746071\n",
      "[8] Test Loss: 0.5078, Accuracy: 81.84%\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.611278\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.697830\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.446573\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.366603\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.549366\n",
      "[9] Test Loss: 0.4975, Accuracy: 82.13%\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.426842\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.502476\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.523116\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.669050\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.510486\n",
      "[10] Test Loss: 0.4786, Accuracy: 82.73%\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.343644\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.608669\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.418264\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.605195\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.523207\n",
      "[11] Test Loss: 0.4665, Accuracy: 83.08%\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.623342\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.540359\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.517885\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.268008\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.360761\n",
      "[12] Test Loss: 0.4638, Accuracy: 83.65%\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.496480\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.499522\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.434076\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.394899\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.371477\n",
      "[13] Test Loss: 0.4521, Accuracy: 84.00%\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.643345\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.516112\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.624568\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.374380\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.537269\n",
      "[14] Test Loss: 0.4566, Accuracy: 83.92%\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.528003\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.415525\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.604496\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.508941\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.423670\n",
      "[15] Test Loss: 0.4421, Accuracy: 84.25%\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.624600\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 0.409530\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.732434\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.483707\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.619154\n",
      "[16] Test Loss: 0.4414, Accuracy: 84.41%\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.537989\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 0.518629\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.330222\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.456258\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.292497\n",
      "[17] Test Loss: 0.4307, Accuracy: 84.60%\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.747275\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 0.548172\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.470589\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 0.426411\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.442242\n",
      "[18] Test Loss: 0.4225, Accuracy: 84.68%\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.505592\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 0.372639\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.288721\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 0.438792\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.373960\n",
      "[19] Test Loss: 0.4209, Accuracy: 84.65%\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.279400\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 0.431130\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.417815\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 0.497652\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.465575\n",
      "[20] Test Loss: 0.4263, Accuracy: 84.63%\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.695839\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 0.463264\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.472669\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 0.526317\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.490370\n",
      "[21] Test Loss: 0.4189, Accuracy: 84.96%\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.378292\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 0.487054\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.654164\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 0.399501\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.402400\n",
      "[22] Test Loss: 0.4127, Accuracy: 85.09%\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.365267\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 0.431604\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.561208\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 0.423235\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.287372\n",
      "[23] Test Loss: 0.4138, Accuracy: 85.06%\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.366155\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 0.437534\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.357295\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 0.439024\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.455659\n",
      "[24] Test Loss: 0.4100, Accuracy: 85.37%\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.592198\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 0.576933\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.464738\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 0.414872\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.758615\n",
      "[25] Test Loss: 0.4092, Accuracy: 85.68%\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.517807\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 0.392671\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 0.397766\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 0.428443\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.507040\n",
      "[26] Test Loss: 0.4100, Accuracy: 85.60%\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.439626\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 0.356242\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 0.291976\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 0.244862\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.448852\n",
      "[27] Test Loss: 0.4002, Accuracy: 85.77%\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.309114\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 0.455504\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 0.405084\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 0.402792\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.478304\n",
      "[28] Test Loss: 0.3996, Accuracy: 85.42%\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.335505\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 0.443026\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 0.401540\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 0.450561\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.394814\n",
      "[29] Test Loss: 0.4005, Accuracy: 85.19%\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.432271\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 0.586472\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 0.565413\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 0.359587\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.352263\n",
      "[30] Test Loss: 0.4061, Accuracy: 85.84%\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 0.193587\n",
      "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 0.368071\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 0.644526\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 0.478820\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 0.489780\n",
      "[31] Test Loss: 0.3943, Accuracy: 85.51%\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 0.318015\n",
      "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 0.375539\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 0.435806\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 0.280759\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 0.475422\n",
      "[32] Test Loss: 0.4037, Accuracy: 85.21%\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 0.401267\n",
      "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 0.469231\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 0.316740\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 0.433955\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 0.363092\n",
      "[33] Test Loss: 0.3949, Accuracy: 85.77%\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 0.264611\n",
      "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 0.518538\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 0.398288\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 0.484223\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 0.403549\n",
      "[34] Test Loss: 0.3967, Accuracy: 85.78%\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.483148\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 0.283006\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 0.435032\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 0.538574\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 0.324166\n",
      "[35] Test Loss: 0.3896, Accuracy: 86.10%\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 0.408083\n",
      "Train Epoch: 36 [12800/60000 (21%)]\tLoss: 0.256318\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 0.255514\n",
      "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 0.395483\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 0.624543\n",
      "[36] Test Loss: 0.3883, Accuracy: 86.06%\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 0.261308\n",
      "Train Epoch: 37 [12800/60000 (21%)]\tLoss: 0.494861\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 0.278020\n",
      "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 0.547508\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 0.380517\n",
      "[37] Test Loss: 0.3899, Accuracy: 85.83%\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 0.472398\n",
      "Train Epoch: 38 [12800/60000 (21%)]\tLoss: 0.651062\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 0.399142\n",
      "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 0.280657\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 0.275238\n",
      "[38] Test Loss: 0.3935, Accuracy: 86.15%\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 0.394987\n",
      "Train Epoch: 39 [12800/60000 (21%)]\tLoss: 0.343345\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 0.725491\n",
      "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 0.443045\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 0.456909\n",
      "[39] Test Loss: 0.3855, Accuracy: 86.10%\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.303468\n",
      "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 0.401982\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 0.376247\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 0.304900\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 0.505898\n",
      "[40] Test Loss: 0.3843, Accuracy: 86.35%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST('./.data',\n",
    "                          train=True,\n",
    "                          download=True,\n",
    "                          transform=transforms.Compose([\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.1307,), (0.3081,))\n",
    "                          ])),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST('./.data',\n",
    "                          train=False,\n",
    "                          transform=transforms.Compose([\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.1307,), (0.3081,))\n",
    "                          ])),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        ## 첫번째 파라미터는 입력 채널의 수이다. Fashion MNIST는 흑백 이미지이므로 1이다.\n",
    "        ## 두번째 파라미터는 출력 채널의 수이다. 10으로 설정했다. (10개의 필터를 사용한다는 의미)\n",
    "        ## stride, padding은 default 값으로 설정.\n",
    "        ## kernel_size(필터의 크기)는 5*5로 설정했다.\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10) ## 분류할 클래스 갯수인 10개로 출력 설정\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        ## 커널사이즈가 2인것은 2*2 윈도우 크기로 맥스 풀링 하겠다는 것\n",
    "        x = F.relu(F.max_pool2d(self.drop(self.conv2(x)), 2))\n",
    "#        print('x사이즈 : ',x.size()) => x사이즈 :  torch.Size([64, 20, 4, 4])\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        ## x사이즈 : 64*10 => 배치사이즈가 64이고, 10개의 클래스로 분류한다는 의미\n",
    "        return x\n",
    "\n",
    "model = CNN().to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.5)\n",
    "\n",
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "\n",
    "            test_loss += F.cross_entropy(output, target,\n",
    "                                         reduction='sum').item()\n",
    "\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "\n",
    "    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
    "        epoch, test_loss, test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. ResNet으로 컬러 데이터셋에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('./.data',\n",
    "                        train=True,\n",
    "                        download=True,\n",
    "                        transform=transforms.Compose([\n",
    "                            transforms.RandomCrop(32, padding=4),\n",
    "                            ## 데이터 증강 방법중 패딩이 추가된 이미지에서 32*32 크기로 랜덤하게 잘라내는 RandomCrop\n",
    "                            ## 채널수는 그대로 가져감 (색상 3개를 쓰는 데이터이므로 채널수는 3 => 32*32*3)\n",
    "                            transforms.RandomHorizontalFlip(),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                (0.5, 0.5, 0.5))\n",
    "                        ])),\n",
    "    batch_size=BATCH_SIZE,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('./.data',\n",
    "                        train=False,\n",
    "                        transform=transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                (0.5, 0.5, 0.5))\n",
    "                        ])),\n",
    "    batch_size=BATCH_SIZE,shuffle=True)\n",
    "\n",
    "## ResNet 내의 기본 블록을 정의 (이건 ResNet 과정 중의 하나이므로 전체 모델 프로세스의 일부라고 생각하고 이해하면 됨.)\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes,kernel_size=3,stride=stride,padding=1,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        ## 출력 채널들을 정규화하는 레이어\n",
    "        self.conv2 = nn.Conv2d(planes, planes,kernel_size=3,stride=stride,padding=1,bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes,kernel_size=1,stride=stride,bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "    \n",
    "## ResNet 모델을 정의 (코드 흐름 이해안되면 책 보기 - 172~177)\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(16, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(32, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(64, 2, stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "    def _make_layer(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(BasicBlock(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "model = ResNet().to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1,\n",
    "                      momentum=0.9, weight_decay=0.0005)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "## 스케줄러는 학습률을 동적으로 조절하는 역할을 한다. (학습률을 조절하는 방법은 여러가지가 있음)\n",
    "## StepLR은 매 step_size마다 learning rate에 gamma를 곱해준다.\n",
    "## 예를 들어, step_size=50, gamma=0.1이면 50번째 epoch마다 learning rate에 0.1을 곱해준다.\n",
    "\n",
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "\n",
    "            test_loss += F.cross_entropy(output, target,\n",
    "                                         reduction='sum').item()\n",
    "\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\higi4\\anaconda3\\envs\\env_deep3book\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (16) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,EPOCHS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      2\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m## 에폭마다 호출해줌\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m] Test Loss: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m      6\u001b[0m         epoch, test_loss, test_accuracy))\n",
      "Cell \u001b[1;32mIn[10], line 97\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, optimizer, epoch)\u001b[0m\n\u001b[0;32m     95\u001b[0m data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(DEVICE), target\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     96\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 97\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(output, target)\n\u001b[0;32m     99\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\higi4\\anaconda3\\envs\\env_deep3book\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\higi4\\anaconda3\\envs\\env_deep3book\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 77\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     75\u001b[0m out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))\n\u001b[0;32m     76\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(out)\n\u001b[1;32m---> 77\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(out)\n\u001b[0;32m     79\u001b[0m out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mavg_pool2d(out, \u001b[38;5;241m8\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\higi4\\anaconda3\\envs\\env_deep3book\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\higi4\\anaconda3\\envs\\env_deep3book\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\higi4\\anaconda3\\envs\\env_deep3book\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\higi4\\anaconda3\\envs\\env_deep3book\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\higi4\\anaconda3\\envs\\env_deep3book\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 49\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     47\u001b[0m out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))\n\u001b[0;32m     48\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out))\n\u001b[1;32m---> 49\u001b[0m \u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshortcut\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (16) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,EPOCHS+1):\n",
    "    scheduler.step() ## 에폭마다 호출해줌\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_deep3book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
